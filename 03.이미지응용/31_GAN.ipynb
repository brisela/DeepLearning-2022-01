{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "31.GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4bgoCUp5Vnu"
      },
      "source": [
        "- 내용 및 코드 참조 \n",
        "\n",
        "  - 케라스 창시자로부터 배우는 딥러닝\n",
        "\n",
        "  - 미술관에 GAN 딥러닝 실전 프로젝트 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeLTZPP-sQ5x"
      },
      "source": [
        "# 적대적 생성 신경망(Generative Adversarial Networks, GAN)\n",
        "\n",
        "- 2014년, Ian Goodfellow 소개\n",
        "\n",
        "- VAE(Variation AutoEncoder)와는 다른 방법으로 이미지 잠재 공간을 학습\n",
        "\n",
        "- 직관적으로 이해하는 방법\n",
        "\n",
        "  - 가짜 피카소 그림을 그리는 위조범과 이를 판별하는 판매상의 관계\n",
        "\n",
        "  - 위조품과 진짜 그림을 섞어서 판매상에게 보여주며  \n",
        "    그림이 진짜인지 가짜인지 판매상은 판별하고 이를 위조범에게 피드백\n",
        "\n",
        "  - 처음에는 형편없는 그림을 그리다가 점점 피카소의 스타일을 모방하게 되고  \n",
        "    판매상은 위조품을 구분하는데 점점 더 전문가가 되어감\n",
        "\n",
        "- GAN의 네트워크\n",
        "\n",
        "  - 생성자 네트워크(generator network)\n",
        "\n",
        "    - 랜덤 벡터(잠재 공간의 무작위한 포인트)를 입력으로 받아 이를 합성된 이미지로 디코딩\n",
        "\n",
        "  - 판별자 네트워크(discriminator network)\n",
        "\n",
        "    - 이미지(실제 또는 가짜 이미지)를 입력으로 받아 훈련 세트에서 온 이미지인지 생성자가 만든 이미지인지 판별\n",
        "\n",
        "- GAN은 최적화의 최솟값이 고정되어 있지 않음\n",
        "\n",
        "  - 보통의 경사하강법은 **고정된 손실공간**에서 언덕을 내려오는 훈련 방법이지만  \n",
        "    GAN은 매 단계가 조금씩 전체 공간을 바꾸기 때문에 최적화 과정이 최솟값을 찾는 것이 어려움\n",
        "\n",
        "  - 두 힘간의 평형점을 찾는 시스템\n",
        "\n",
        "  - 따라서, 학습과정이 매우 어려움  \n",
        "    즉, 적절한 파라미터를 찾고 조정해야함\n",
        "  \n",
        "  <img src=\"https://paperswithcode.com/media/methods/gan.jpeg\">\n",
        "\n",
        "  <sub>[이미지 출처] https://paperswithcode.com/method/gan</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHhtFZ1huWoB"
      },
      "source": [
        "## GAN의 구현 (DCGAN: Deep Convolutional Generative Adversarial Networks)\n",
        "\n",
        "- CIFAR-10 데이터셋 사용\n",
        "\n",
        "- generator 네트워크는 (latent_dim,) 크기의 벡터를 (32, 32, 3) 크기의 이미지로 매핑\n",
        "\n",
        "- discriminator 네트워크는 (32, 32, 3) 크기의 이미지가 진짜일 확률을 추정하여 이진값으로 매핑\n",
        "\n",
        "- 생성자와 판별자를 연결하는 gan 네트워크를 만듬  \n",
        "  \n",
        "  - gan(x) = discriminator(generator(x))\n",
        "\n",
        "- 진짜/가짜 레이블과 함께 진짜 이미지와 가짜 이미지 샘플을 사용하여 판별자를 훈련 (일반적인 이미지 분류 모델 훈련과 동일)\n",
        "\n",
        "- 생성자를 훈련하려면 gan 모델의 손실에 대한 생성자 가중치의 그래디언트를 사용\n",
        "\n",
        "  - 매 단계마다 생성자에 의해 디코딩된 이미지를 판별자가 \"진짜\"로 분류하도록 만드는 방향으로 생성자의 가중치를 이동\n",
        "\n",
        "  - 판별자를 속이도록 생성자를 훈련한다는 말\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb4JHajh1gXx"
      },
      "source": [
        "## 학습 방법\n",
        "\n",
        "- 마지막 활성화 함수로 sigmoid 대신 tanh함수 사용\n",
        "\n",
        "- 균등분포가 아니고 정규 분포(가우시안 분포)를 사용하여 잠재 공간에서 포인트를 샘플링\n",
        "\n",
        "- 무작위성을 주입\n",
        "\n",
        "  - 판별자에 드롭아웃을 사용\n",
        "\n",
        "  - 판별자를 위해 레이블에 랜덤 노이즈를 추가\n",
        "\n",
        "- 희소한 그래디언트는 GAN 훈련에 방해가 될 수 있음\n",
        "\n",
        "  - 최대 풀링 대신 스트라이드 합성곱을 사용하여 다운샘플링\n",
        "\n",
        "  - ReLU 대신 LeakyReLU 사용\n",
        "\n",
        "- 생성자에서 픽셀 공간을 균일하게 다루지 못하여 생성된 이미지에서 체스판 모양이 종종 나타남.  \n",
        "  이를 위해 생성자와 판별자에서 스트라이드 Conv2DTranspose나 Conv2D를 사용할 때 스트라이드 크기로 나누어 질 수 있는 커널 크기 사용\n",
        "\n",
        "  - 커널 크기가 스트라이드의 배수가 아니면 픽셀이 공평하게 합성곱 되지 않음  \n",
        "    커널 크기를 스트라이드로 나누었을 때 나머지 크기에 해당하는 픽셀이 더 많이 업샘플링에 참여하게 됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiIV25ew2SDq"
      },
      "source": [
        "### GAN 생성자 네트워크\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-y52HdbXSkq"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Reshape, Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkWJCAGF2o0r"
      },
      "source": [
        "latent_dim = 32\n",
        "height, width, channels = 32, 32, 3"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVp1CdYG2tu0"
      },
      "source": [
        "generator_input = Input(shape=(latent_dim,))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI2bVCCz2wgh"
      },
      "source": [
        "x = Dense(128 * 16 * 16)(generator_input)\n",
        "x = LeakyReLU()(x)\n",
        "x = Reshape((16, 16, 128))(x)\n",
        "\n",
        "x = Conv2D(256, 5, padding='same')(x)\n",
        "x = LeakyReLU()(x)\n",
        "\n",
        "x = Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
        "x = LeakyReLU()(x)\n",
        "\n",
        "x = Conv2D(256, 5, padding='same')(x)\n",
        "x = LeakyReLU()(x)\n",
        "x = Conv2D(256, 5, padding='same')(x)\n",
        "x = LeakyReLU()(x)\n",
        "\n",
        "x = Conv2D(channels, 7, activation='tanh', padding='same')(x)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Model(generator_input, x)\n",
        "generator.summary()"
      ],
      "metadata": {
        "id": "wovZ-4oz2Ehb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5359cf6b-1666-4ef2-f656-8f84fece4db4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32)]              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32768)             1081344   \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 32768)             0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 16, 16, 256)       819456    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 32, 32, 256)      1048832   \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 256)       1638656   \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 32, 32, 256)       1638656   \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 32, 32, 3)         37635     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,264,579\n",
            "Trainable params: 6,264,579\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnRjKuGy3By9"
      },
      "source": [
        "### GAN 판별자 네트워크\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0rd_EYq3Epn"
      },
      "source": [
        "discriminator_input = Input(shape=(height, width, channels))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcJ8iXrO3BXO"
      },
      "source": [
        "x = Conv2D(128, 3)(discriminator_input)\n",
        "x = LeakyReLU()(x)\n",
        "x = Conv2D(128, 4, strides=2)(x)\n",
        "x = LeakyReLU()(x)\n",
        "x = Conv2D(128, 4, strides=2)(x)\n",
        "x = LeakyReLU()(x)\n",
        "x = Conv2D(128, 4, strides=2)(x)\n",
        "x = LeakyReLU()(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(1, activation='sigmoid')(x)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = Model(discriminator_input, x)\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "id": "MiByLWVh2Hxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c314ccc-6173-4534-944e-11964e0422e8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 30, 30, 128)       3584      \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 30, 30, 128)       0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 14, 14, 128)       262272    \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 6, 6, 128)         262272    \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 6, 6, 128)         0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 2, 2, 128)         262272    \n",
            "                                                                 \n",
            " leaky_re_lu_8 (LeakyReLU)   (None, 2, 2, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 790,913\n",
            "Trainable params: 790,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQuMOoH_3WbY"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3zZbyx13TGi"
      },
      "source": [
        "discriminator_optimizer = RMSprop(learning_rate=0.0008, clipvalue=1.0, decay=1e-8)\n",
        "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayIotK_Q3iE7"
      },
      "source": [
        "### 적대적 네트워크\n",
        "- 생성자와 판별자를 연결\n",
        "\n",
        "- 훈련할 때 생성자가 판별자를 속이는 능력이 커지도록 학습\n",
        "\n",
        "- 잠재 공간의 포인트를 진짜 또는 가짜의 분류 결정으로 변환\n",
        "\n",
        "- 훈련에 사용되는 target label은 항상 \"진짜 이미지\"\n",
        "\n",
        "- 훈련하는 동안 판별자를 동결(학습되지 않도록)하는 것이 매우 중요!\n",
        "\n",
        "  - 판별자의 가중치가 훈련하는 동안 업데이트되면 판별자는 항상 진짜를 예측하도록 훈련됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eerQWi8_3auo"
      },
      "source": [
        "discriminator.trainable = False\n",
        "\n",
        "gan_input = Input(shape=(latent_dim,))\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = Model(gan_input, gan_output)\n",
        "\n",
        "gan_optimizer = RMSprop(learning_rate=0.0004, clipvalue=1.0, decay=1e-8)\n",
        "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gan.summary()"
      ],
      "metadata": {
        "id": "07JdYkoq2NW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee898df-660a-4e67-ca27-87eba4aa3708"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 32)]              0         \n",
            "                                                                 \n",
            " model (Functional)          (None, 32, 32, 3)         6264579   \n",
            "                                                                 \n",
            " model_1 (Functional)        (None, 1)                 790913    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,055,492\n",
            "Trainable params: 6,264,579\n",
            "Non-trainable params: 790,913\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP5cjQs74G5Q"
      },
      "source": [
        "### GAN 훈련 구현\n",
        "\n",
        "- DCGAN 구현\n",
        "\n",
        "- 매 반복마다 아래의 과정 수행\n",
        "\n",
        "  1. 잠재 공간에서 무작위로 포인트를 뽑음(랜덤 노이즈)\n",
        "\n",
        "  2. 랜덤 노이즈를 사용하여 generator에서 이미지를 생성\n",
        "\n",
        "  3. 생성된 이미지와 진짜 이미지를 섞음\n",
        "\n",
        "  4. 진짜와 가짜가 섞인 이미지와 이에 대응하는 타깃을 사용하여 discriminator를 훈련  \n",
        "    타킷은 진짜 또는 가짜\n",
        "\n",
        "  5. 잠재 공간에서 무작위로 새로운 포인트를 뽑음\n",
        "\n",
        "  6. 랜덤 벡터를 사용하여 gan을 훈련.  \n",
        "    모든 타깃은 진짜로 설정. \n",
        "    - 판별자가 생성된 이미지를 모두 \"진짜 이미지\"라고 예측하도록 생성자의 가중치를 업데이트.  \n",
        "      (판별자는 동결되기 때문에 생성자만 업데이트)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GimAaUDC4u-P"
      },
      "source": [
        "import os\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing import image"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLrdG7a84yFE"
      },
      "source": [
        "#### CIFAR10 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siDPaZ-J3nSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10af0f23-ef4f-4aec-fcfd-d0e31fb37148"
      },
      "source": [
        "(X_train, y_train), (_, _) = cifar10.load_data()\n",
        "X_train.shape, y_train.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 32, 32, 3), (50000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgzNpE6O4-ik"
      },
      "source": [
        "#### 개구리 이미지를 선택"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBF8XEbj41ZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43eedf7b-c446-4c97-f29e-31d1c573c26b"
      },
      "source": [
        "X_train = X_train[y_train.flatten() == 6]\n",
        "X_train.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be7fKh6-5BMn"
      },
      "source": [
        "#### 데이터 정규화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b12T22Sh5FQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ee64b9-521f-438d-bbe6-f4f18ceef34d"
      },
      "source": [
        "X_train = X_train.astype('float32') / 255.\n",
        "type(X_train[0,0,0,0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.float32"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 10000\n",
        "batch_size = 20\n",
        "\n",
        "save_dir = 'gan_images'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.mkdir(save_dir)"
      ],
      "metadata": {
        "id": "A0mVnALctuNL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIfCHuXQ7HOJ"
      },
      "source": [
        "#### 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puOujZ3F5D2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f551ea1-eeca-4581-e075-000fdd5e91e7"
      },
      "source": [
        "start = 0\n",
        "for step in range(iterations):\n",
        "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
        "    generated_images = generator.predict(random_latent_vectors)\n",
        "\n",
        "    stop = start + batch_size\n",
        "    real_images = X_train[start:stop]\n",
        "    combined_images = np.concatenate([generated_images, real_images])\n",
        "\n",
        "    labels = np.concatenate([np.ones((batch_size,1)), np.zeros((batch_size,1))])\n",
        "    labels += 0.05 * np.random.random(labels.shape)\n",
        "\n",
        "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
        "\n",
        "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
        "    misleading_targets = np.zeros((batch_size, 1))\n",
        "\n",
        "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
        "\n",
        "    start += batch_size\n",
        "    if start > len(X_train) - batch_size:\n",
        "        start = 0\n",
        "    \n",
        "    if step % 100 == 0:\n",
        "        print(f'Step: {step}, discriminator loss: {d_loss:.4f}, adversarial loss: {a_loss:.4f}')\n",
        "        gan.save_weights('gan.h5')\n",
        "        img = image.array_to_img(generated_images[0] * 255, scale=False)\n",
        "        img.save(os.path.join(save_dir, f'generated_frog_{step}.png'))\n",
        "        img = image.array_to_img(real_images[0] * 255, scale=False)\n",
        "        img.save(os.path.join(save_dir, f'real_frog_{step}.png'))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0, discriminator loss: 0.6858, adversarial loss: 0.6720\n",
            "Step: 100, discriminator loss: 0.0490, adversarial loss: 6.2389\n",
            "Step: 200, discriminator loss: -2.5422, adversarial loss: 996.3830\n",
            "Step: 300, discriminator loss: -6.1846, adversarial loss: 2994.8562\n",
            "Step: 400, discriminator loss: -27.6645, adversarial loss: 2687.8840\n",
            "Step: 500, discriminator loss: -149.8607, adversarial loss: 8984.0723\n",
            "Step: 600, discriminator loss: -102.3769, adversarial loss: 26823.6875\n",
            "Step: 700, discriminator loss: -658.1179, adversarial loss: 53934.7734\n",
            "Step: 800, discriminator loss: 969.7719, adversarial loss: 154447.4531\n",
            "Step: 900, discriminator loss: -4698.3438, adversarial loss: 3163205.5000\n",
            "Step: 1000, discriminator loss: -125929.7500, adversarial loss: 9650623.0000\n",
            "Step: 1100, discriminator loss: -160187.5000, adversarial loss: 16234688.0000\n",
            "Step: 1200, discriminator loss: -489057.5938, adversarial loss: 61266676.0000\n",
            "Step: 1300, discriminator loss: 10579.0000, adversarial loss: 65381332.0000\n",
            "Step: 1400, discriminator loss: -2249038.5000, adversarial loss: 167782800.0000\n",
            "Step: 1500, discriminator loss: -3076718.0000, adversarial loss: 303740064.0000\n",
            "Step: 1600, discriminator loss: -7308940.0000, adversarial loss: 603811712.0000\n",
            "Step: 1700, discriminator loss: -8245597.5000, adversarial loss: 440836192.0000\n",
            "Step: 1800, discriminator loss: -4448310.0000, adversarial loss: 635920768.0000\n",
            "Step: 1900, discriminator loss: -10955369.0000, adversarial loss: 1012101440.0000\n",
            "Step: 2000, discriminator loss: -14454280.0000, adversarial loss: 1820977536.0000\n",
            "Step: 2100, discriminator loss: -18835058.0000, adversarial loss: 2616139008.0000\n",
            "Step: 2200, discriminator loss: -38900328.0000, adversarial loss: 3430650368.0000\n",
            "Step: 2300, discriminator loss: -4936329.5000, adversarial loss: 2719308288.0000\n",
            "Step: 2400, discriminator loss: -61822860.0000, adversarial loss: 7117588992.0000\n",
            "Step: 2500, discriminator loss: -69898584.0000, adversarial loss: 7788327936.0000\n",
            "Step: 2600, discriminator loss: -80554528.0000, adversarial loss: 8524876288.0000\n",
            "Step: 2700, discriminator loss: -102720208.0000, adversarial loss: 13171578880.0000\n",
            "Step: 2800, discriminator loss: -4428928.0000, adversarial loss: 11583223808.0000\n",
            "Step: 2900, discriminator loss: -267582416.0000, adversarial loss: 18370113536.0000\n",
            "Step: 3000, discriminator loss: -226886176.0000, adversarial loss: 30363521024.0000\n",
            "Step: 3100, discriminator loss: -184502320.0000, adversarial loss: 24509112320.0000\n",
            "Step: 3200, discriminator loss: -297718912.0000, adversarial loss: 36058906624.0000\n",
            "Step: 3300, discriminator loss: -330554560.0000, adversarial loss: 34826084352.0000\n",
            "Step: 3400, discriminator loss: -380547488.0000, adversarial loss: 51860496384.0000\n",
            "Step: 3500, discriminator loss: -584355456.0000, adversarial loss: 63492009984.0000\n",
            "Step: 3600, discriminator loss: -690346304.0000, adversarial loss: 72775450624.0000\n",
            "Step: 3700, discriminator loss: -1081077376.0000, adversarial loss: 92527476736.0000\n",
            "Step: 3800, discriminator loss: 138692816.0000, adversarial loss: 95087435776.0000\n",
            "Step: 3900, discriminator loss: -531722656.0000, adversarial loss: 127350784000.0000\n",
            "Step: 4000, discriminator loss: -1664008960.0000, adversarial loss: 143276392448.0000\n",
            "Step: 4100, discriminator loss: -1711719424.0000, adversarial loss: 151986978816.0000\n",
            "Step: 4200, discriminator loss: -1570281216.0000, adversarial loss: 167221821440.0000\n",
            "Step: 4300, discriminator loss: -9797018.0000, adversarial loss: 167310229504.0000\n",
            "Step: 4400, discriminator loss: -2536452864.0000, adversarial loss: 254866276352.0000\n",
            "Step: 4500, discriminator loss: -3054470144.0000, adversarial loss: 321143275520.0000\n",
            "Step: 4600, discriminator loss: -3588582144.0000, adversarial loss: 320529924096.0000\n",
            "Step: 4700, discriminator loss: -4002395392.0000, adversarial loss: 421525094400.0000\n",
            "Step: 4800, discriminator loss: -1799111424.0000, adversarial loss: 326115950592.0000\n",
            "Step: 4900, discriminator loss: -6557778432.0000, adversarial loss: 505069502464.0000\n",
            "Step: 5000, discriminator loss: -9124395008.0000, adversarial loss: 736310394880.0000\n",
            "Step: 5100, discriminator loss: -7149050368.0000, adversarial loss: 892753936384.0000\n",
            "Step: 5200, discriminator loss: -10003361792.0000, adversarial loss: 947319209984.0000\n",
            "Step: 5300, discriminator loss: -1325231360.0000, adversarial loss: 655392964608.0000\n",
            "Step: 5400, discriminator loss: -6388644352.0000, adversarial loss: 1161557049344.0000\n",
            "Step: 5500, discriminator loss: -20187279360.0000, adversarial loss: 1640135655424.0000\n",
            "Step: 5600, discriminator loss: -11405978624.0000, adversarial loss: 1298224119808.0000\n",
            "Step: 5700, discriminator loss: -20657926144.0000, adversarial loss: 2064783769600.0000\n",
            "Step: 5800, discriminator loss: 7122611200.0000, adversarial loss: 1216788824064.0000\n",
            "Step: 5900, discriminator loss: -9269487616.0000, adversarial loss: 1953549123584.0000\n",
            "Step: 6000, discriminator loss: -21938939904.0000, adversarial loss: 1980278112256.0000\n",
            "Step: 6100, discriminator loss: -20839593984.0000, adversarial loss: 2219715330048.0000\n",
            "Step: 6200, discriminator loss: -24980953088.0000, adversarial loss: 2400638730240.0000\n",
            "Step: 6300, discriminator loss: 5820052480.0000, adversarial loss: 2268550660096.0000\n",
            "Step: 6400, discriminator loss: -34148067328.0000, adversarial loss: 3509362884608.0000\n",
            "Step: 6500, discriminator loss: -54490206208.0000, adversarial loss: 4656698228736.0000\n",
            "Step: 6600, discriminator loss: -32807753728.0000, adversarial loss: 3519918637056.0000\n",
            "Step: 6700, discriminator loss: -40194252800.0000, adversarial loss: 3851221729280.0000\n",
            "Step: 6800, discriminator loss: -1038123008.0000, adversarial loss: 3839389597696.0000\n",
            "Step: 6900, discriminator loss: -60041691136.0000, adversarial loss: 6198453075968.0000\n",
            "Step: 7000, discriminator loss: -91225456640.0000, adversarial loss: 7173333581824.0000\n",
            "Step: 7100, discriminator loss: -35654676480.0000, adversarial loss: 4698828963840.0000\n",
            "Step: 7200, discriminator loss: -64988008448.0000, adversarial loss: 6135691083776.0000\n",
            "Step: 7300, discriminator loss: -21846878208.0000, adversarial loss: 5128265400320.0000\n",
            "Step: 7400, discriminator loss: -106590846976.0000, adversarial loss: 9557447278592.0000\n",
            "Step: 7500, discriminator loss: -99289907200.0000, adversarial loss: 9844426801152.0000\n",
            "Step: 7600, discriminator loss: -105778380800.0000, adversarial loss: 9606489178112.0000\n",
            "Step: 7700, discriminator loss: -121943179264.0000, adversarial loss: 12784620273664.0000\n",
            "Step: 7800, discriminator loss: 4159687936.0000, adversarial loss: 12519421771776.0000\n",
            "Step: 7900, discriminator loss: -121739722752.0000, adversarial loss: 14812718301184.0000\n",
            "Step: 8000, discriminator loss: -159814598656.0000, adversarial loss: 14377481666560.0000\n",
            "Step: 8100, discriminator loss: -159017041920.0000, adversarial loss: 13657194889216.0000\n",
            "Step: 8200, discriminator loss: -138604609536.0000, adversarial loss: 12546109079552.0000\n",
            "Step: 8300, discriminator loss: -46614642688.0000, adversarial loss: 11142741098496.0000\n",
            "Step: 8400, discriminator loss: -158773575680.0000, adversarial loss: 16903137394688.0000\n",
            "Step: 8500, discriminator loss: -242782126080.0000, adversarial loss: 23074479013888.0000\n",
            "Step: 8600, discriminator loss: -344531566592.0000, adversarial loss: 26690877128704.0000\n",
            "Step: 8700, discriminator loss: -335936618496.0000, adversarial loss: 33261317259264.0000\n",
            "Step: 8800, discriminator loss: -342931013632.0000, adversarial loss: 36908558188544.0000\n",
            "Step: 8900, discriminator loss: -466242404352.0000, adversarial loss: 41610456858624.0000\n",
            "Step: 9000, discriminator loss: -311325229056.0000, adversarial loss: 33231996977152.0000\n",
            "Step: 9100, discriminator loss: -244434337792.0000, adversarial loss: 27797863006208.0000\n",
            "Step: 9200, discriminator loss: -218738556928.0000, adversarial loss: 19594945232896.0000\n",
            "Step: 9300, discriminator loss: -31770253312.0000, adversarial loss: 21318107922432.0000\n",
            "Step: 9400, discriminator loss: -345421709312.0000, adversarial loss: 33806251720704.0000\n",
            "Step: 9500, discriminator loss: -364652593152.0000, adversarial loss: 35682152087552.0000\n",
            "Step: 9600, discriminator loss: -224933707776.0000, adversarial loss: 29700950851584.0000\n",
            "Step: 9700, discriminator loss: -349237182464.0000, adversarial loss: 38399633260544.0000\n",
            "Step: 9800, discriminator loss: 160545685504.0000, adversarial loss: 34834416140288.0000\n",
            "Step: 9900, discriminator loss: -572365078528.0000, adversarial loss: 50590990204928.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I4J9HSI7Kco"
      },
      "source": [
        "#### 이미지 생성을 통한 시각화\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9r2Bfwk5iNf"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS9f7FGO7ijf"
      },
      "source": [
        "- 잠재 공간에서 랜덤한 포인트를 샘플링\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6zPuYw97EHk"
      },
      "source": [
        "random_latent_vectors = np.random.normal(size=(10, latent_dim))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IikNB0zM7lQ8"
      },
      "source": [
        "- 가짜 이미지로 디코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHSY6hNU7AlE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a03b092-568b-4a19-b6b2-599f592708b5"
      },
      "source": [
        "generated_images = generator.predict(random_latent_vectors)\n",
        "generated_images.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gyn_d90--r0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "ec039836-e402-4a3e-9fa2-56546b5e0fdc"
      },
      "source": [
        "plt.figure(figsize=(5,2))\n",
        "for i in range(generated_images.shape[0]):\n",
        "    plt.subplot(2, 5, i+1)\n",
        "    img = image.array_to_img(generated_images[i] * 255, scale=False)\n",
        "    plt.imshow(img)\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x144 with 10 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip images.zip gan_images/* > /dev/null"
      ],
      "metadata": {
        "id": "LKbawPXr3XNn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BwocriqA4tWG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}